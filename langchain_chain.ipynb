{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccd79b0",
   "metadata": {},
   "source": [
    "# チェーンとLCEL\n",
    "\n",
    "## 1. 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なモジュールをインポート\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# モデル名\n",
    "MODEL_NAME = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a517971",
   "metadata": {},
   "source": [
    "## 2.基本のチェーン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは{animal}らしく、語尾に{voice}などと付けて答えます。\"),\n",
    "    (\"human\", \"{question}をする上でのポイントは？\"),\n",
    "])\n",
    "\n",
    "# モデルの作成\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# 文字列で出力\n",
    "str_output_parser = StrOutputParser()\n",
    "\n",
    "# チェーンの作成。それぞれはRunnableである\n",
    "chain = prompt | model | str_output_parser\n",
    "\n",
    "# チェーンの実行\n",
    "response = chain.invoke({\"animal\": \"犬\", \"voice\": \"ワン！\", \"question\": \"英語学習\"})\n",
    "\n",
    "# 結果を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96483d",
   "metadata": {},
   "source": [
    "## 3.任意の関数をチェーンにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# 与えられた文字列を逆順にして返す関数。@chainデコレータでRunnableLambdaになる\n",
    "@chain\n",
    "def reverse_string(message):\n",
    "    return message[::-1]  # 文字列を反転\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{question}\\n\\n上記の質問について簡潔に回答してください。\"),\n",
    "])\n",
    "\n",
    "# 文字列で出力\n",
    "str_output_parser = StrOutputParser()\n",
    "\n",
    "# チェーンの作成。@chainの代わりにここでRunnableLambda(reverse_string)としても良い。またはチェーンの一部なら自動変換される\n",
    "my_chain = prompt | model | str_output_parser | reverse_string\n",
    "\n",
    "# チェーンの実行\n",
    "response = my_chain.invoke({\"question\": \"こんにちは！\"})\n",
    "\n",
    "# 結果を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c5152",
   "metadata": {},
   "source": [
    "## 4.複数のチェーンを並列につなげる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "positive_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは楽観主義者です。ユーザーからの質問に対して常に前向きな回答をします。\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "negative_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは悲観主義者です。ユーザーからの質問に対して常に否定的な回答をします。\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# モデルの作成\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# チェーンの作成\n",
    "positive_chain = positive_prompt | model | str_output_parser\n",
    "negative_chain = negative_prompt | model | str_output_parser\n",
    "# 並行チェーンの作成\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"positive\": positive_chain,\n",
    "    \"negative\": negative_chain\n",
    "})\n",
    "\n",
    "# チェーンの実行\n",
    "response = parallel_chain.invoke({\"question\": \"AIの進化が人間に与える影響は？\"})\n",
    "\n",
    "# 結果を表示\n",
    "print(json.dumps(response, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 意見をまとめるプロンプトテンプレートの作成\n",
    "opinion_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは平等主義者です。2つの意見を平等にまとめます。まとめた結果だけを出力します。\"),\n",
    "    (\"human\", \"楽観的な意見: {positive}\\n悲観的な意見: {negative}\"),\n",
    "])\n",
    "\n",
    "opinion_chain = parallel_chain | opinion_prompt | model | str_output_parser\n",
    "\n",
    "# チェーンの実行\n",
    "response = opinion_chain.invoke({\"question\": \"AIの進化が人間に与える影響は？\"})\n",
    "\n",
    "# 結果を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb13f1c",
   "metadata": {},
   "source": [
    "## 5.入力をそのまま出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f40411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "improve_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ユーザーからの質問を分析し、条件を定め、目的を明確にしたうえで、必要であればマークダウン記法を使い、タスクを細分化して最適な出力が得られるプロンプトを考えてください。最終的なプロンプトのみを出力してください。「ユーザーからの質問」に回答しないでください。\"),\n",
    "    (\"human\", \"質問: {prompt}\"),\n",
    "])\n",
    "\n",
    "# モデルの作成\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# プロンプト改善チェーンの作成\n",
    "prompt_chain = improve_prompt | model | str_output_parser\n",
    "\n",
    "# 並行チェーンの作成\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"original\": RunnablePassthrough(),\n",
    "    \"improve\": prompt_chain\n",
    "})\n",
    "\n",
    "# チェーンの実行\n",
    "response = parallel_chain.invoke({\"prompt\": \"AIの進化が人間に与える影響は？\"})\n",
    "\n",
    "# 結果を表示\n",
    "print(json.dumps(response, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d12a3",
   "metadata": {},
   "source": [
    "## 6.ツールの使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# ツールの初期化\n",
    "tavily_search = TavilySearch(\n",
    "    max_results=2,                 # 取得する検索結果の数（k=5と同じ）\n",
    "    search_depth=\"basic\",          # \"basic\" (高速) か \"advanced\" (高品質)\n",
    "    include_answer=False,          # Tavilyが生成した短い回答を含めない\n",
    "    include_raw_content=False,     # HTMLの生コンテンツを含めるか（トークン消費増に注意）\n",
    "    include_images=False,          # 画像URLを含めるか\n",
    "    # include_domains=[\"go.jp\"],   # 特定のドメインのみ検索する場合\n",
    "    # exclude_domains=[\"wikipedia.org\"] # 特定のドメインを除外する場合\n",
    ")\n",
    "\n",
    "# Tavilyの検索結果だけを、LLMが読みやすい形に整形する関数\n",
    "def format_tavily_results(tavily_response: dict) -> str:\n",
    "    results = tavily_response.get(\"results\", [])\n",
    "    if not results:\n",
    "        return \"（検索結果なし）\"\n",
    "\n",
    "    lines = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        title = r.get(\"title\", \"\")\n",
    "        content = r.get(\"content\", \"\")\n",
    "        url = r.get(\"url\", \"\")\n",
    "        lines.append(f\"[{i}] {title}\\n{content}\\nsource: {url}\")\n",
    "    return \"\\n\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632862f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tavily searchツールのテスト\n",
    "response = tavily_search.invoke(\"奈良県のお土産と言えば？\")\n",
    "print(json.dumps(response, ensure_ascii=False, indent=2))\n",
    "\n",
    "# セパレーター\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 整形後の結果を確認\n",
    "print(format_tavily_results(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Tavily → results整形のチェーン\n",
    "context_chain = tavily_search | format_tavily_results\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "あなたは以下の「知識」にのみ基づいて、ユーザーの質問に回答してください。\n",
    "知識に書かれていない内容は推測せず、「知識からは分かりません」と答えてください。\n",
    "可能であれば、回答の末尾に参照したsource番号（例：[1][2]）を付けてください。\n",
    "最後にsource番号とurlを箇条書きで列挙してください。\n",
    "\n",
    "# 知識:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_text),\n",
    "    (\"human\", \"質問: {question}\"),\n",
    "])\n",
    "\n",
    "# モデルの作成\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": context_chain,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | prompt | model | str_output_parser\n",
    ")\n",
    "\n",
    "# チェーンの実行\n",
    "response = chain.invoke(\"奈良県のお土産と言えば？\")\n",
    "\n",
    "# 結果を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653059b8",
   "metadata": {},
   "source": [
    "## 7.条件分岐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3717dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.router import RouterRunnable\n",
    "from pydantic import BaseModel, Field # 構造化データ定義用\n",
    "\n",
    "# 判定ロジック用のクラス定義\n",
    "class SearchDecision(BaseModel):\n",
    "    \"\"\"検索が必要かどうかを判定するためのモデル\"\"\"\n",
    "    needs_search: bool = Field(description=\"Web検索が必要な場合はTrue、不要な場合はFalse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索が必要かどうかを判断するチェーン\n",
    "needs_search_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{question}\\n\\nこの質問に答えるためにWeb検索が必要ですか？\")\n",
    "])\n",
    "\n",
    "needs_search_chain = (\n",
    "    needs_search_prompt \n",
    "    | model.with_structured_output(SearchDecision) \n",
    "    | (lambda x: \"search\" if x.needs_search else \"no_search\") # 判定結果をルーティング用のkeyに変換\n",
    ")\n",
    "\n",
    "# 検索を実行するチェーン\n",
    "context_chain = tavily_search | format_tavily_results\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "あなたは以下の「知識」にのみ基づいて、ユーザーの質問に回答してください。\n",
    "知識に書かれていない内容は推測せず、「知識からは分かりません」と答えてください。\n",
    "可能であれば、回答の末尾に参照したsource番号（例：[1][2]）を付けてください。\n",
    "最後にsource番号とurlを箇条書きで列挙してください。\n",
    "\n",
    "# 知識:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "search_answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_text),\n",
    "    (\"human\", \"質問: {question}\"),\n",
    "])\n",
    "\n",
    "search_chain = (\n",
    "    {\"context\": context_chain, \n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | search_answer_prompt | model | str_output_parser\n",
    ")\n",
    "\n",
    "# 検索なしで回答するチェーン\n",
    "no_search_answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{question}\\n\\n上記の質問に答えてください。\")\n",
    "])\n",
    "\n",
    "no_search_chain = (\n",
    "    no_search_answer_prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RouterRunnable による分岐\n",
    "# key に対応する Runnable を実行する\n",
    "router = RouterRunnable(\n",
    "    runnables={\n",
    "        \"search\": search_chain,\n",
    "        \"no_search\": no_search_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "# RouterRunnable の入力は {\"key\": \"...\", \"input\": ...} の形\n",
    "final_chain = (\n",
    "    {\"key\": needs_search_chain, \"input\": RunnablePassthrough()}\n",
    "    | router\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ff1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 実行: iphone16eの価格は？ ---\")\n",
    "# needs_search_chain が \"search\" を返す -> search_chain が実行される\n",
    "print(final_chain.invoke(\"iphone16eの価格は？\"))\n",
    "\n",
    "print(\"\\n--- 実行: 1+1は？ ---\")\n",
    "# needs_search_chain が \"no_search\" を返す -> no_search_chain が実行される\n",
    "print(final_chain.invoke(\"1+1は？\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
