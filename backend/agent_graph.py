# backend/agent_graph.py
from __future__ import annotations

import uuid
from typing import Annotated, Literal
from typing_extensions import TypedDict

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_tavily import TavilySearch
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, BaseMessage

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
import os
import sqlite3
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.types import Command, interrupt

load_dotenv("../.env")

MODEL_NAME = "gpt-5-mini"

DEBUG_MODE = True


def print_debug(title: str, content: str) -> None:
    if DEBUG_MODE:
        print(f"\nðŸ› [DEBUG] {title}:\n{content}\n" + "-" * 40)


# ----------------------------
# Tools
# ----------------------------
tavily_search = TavilySearch(
    max_results=2,
    search_depth="basic",
    include_answer=False,
    include_raw_content=False,
    include_images=False,
)


def format_tavily_results(tavily_response: dict) -> str:
    results = tavily_response.get("results", [])
    if not results:
        return "ï¼ˆæ¤œç´¢çµæžœãªã—ï¼‰"

    lines = []
    for i, r in enumerate(results, 1):
        title = r.get("title", "")
        # contentãŒè–„ã„ã‚±ãƒ¼ã‚¹ã«å‚™ãˆã¦ raw_content ã‚‚æ‹¾ã†ï¼ˆè¨­å®šæ¬¡ç¬¬ï¼‰
        content = r.get("content") or r.get("raw_content") or ""
        url = r.get("url", "")
        lines.append(f"[{i}] {title}\n{content}\nsource: {url}")
    return "\n\n".join(lines)


@tool
def tavily_search_formatted(query: str) -> str:
    """Webæ¤œç´¢ï¼ˆTavilyï¼‰ã€‚ä¸Šä½çµæžœã‚’æ•´å½¢ã—ã¦è¿”ã™ã€‚"""
    tavily_response = tavily_search.invoke({"query": query})
    return format_tavily_results(tavily_response)


tools = [tavily_search_formatted]


# ----------------------------
# State
# ----------------------------
class State(TypedDict):
    research_messages: Annotated[list[BaseMessage], add_messages]
    analysis_messages: Annotated[list[BaseMessage], add_messages]
    loop_count: int


# ----------------------------
# Model
# ----------------------------
model = ChatOpenAI(model=MODEL_NAME)
model_with_tools = model.bind_tools(tools)


# ----------------------------
# Nodes
# ----------------------------
MAX_TOOL_LOOPS = 3

research_prompt_text = """
ã‚ãªãŸã¯äº‹æ¥­ãƒªã‚µãƒ¼ãƒæ‹…å½“ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ†ãƒ¼ãƒžã«ã¤ã„ã¦ã€å¸‚å ´è¦æ¨¡ã€ä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€æŠ€è¡“èª²é¡Œãªã©ã‚’Webæ¤œç´¢ã§èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚å¿…è¦ã«å¿œã˜ã¦æœ€é©ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å¾Œã«æ–‡ç« ã§ã¾ã¨ã‚ã‚‹å ´åˆã¯ã€ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ã«ã‚ã‚‹ source: URL ã‚’æ ¹æ‹ ã¨ã—ã¦ä½¿ã†ã¨ãã®ã¿ [n] ã‚’ä»˜ã‘ã€æœ«å°¾ã«å‚ç…§ä¸€è¦§ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚å­˜åœ¨ã—ãªã„å‡ºå…¸ã¯ä½œã‚‰ãªã„ã“ã¨ã€‚
"""

research_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", research_prompt_text),
        MessagesPlaceholder(variable_name="research_messages"),
    ]
)
research_chain = research_prompt | model_with_tools


def research_agent(state: State) -> Command[Literal["tools", "summary_agent"]]:
    print_debug("Node", "research_agent")
    response = research_chain.invoke({"research_messages": state["research_messages"]})
    update = {"research_messages": [response]}
    current_count = state.get("loop_count", 0)

    if getattr(response, "tool_calls", None):
        if current_count < MAX_TOOL_LOOPS:
            return Command(update=update, goto="tools")
        return Command(update=update, goto="summary_agent")

    return Command(update=update, goto="summary_agent")


tool_node = ToolNode(tools, messages_key="research_messages")


def research_tool_node(state: State) -> Command[Literal["research_agent"]]:
    result = tool_node.invoke({"research_messages": state["research_messages"]})

    last_message = result["research_messages"][-1]
    tool_text = last_message.content
    tool_text = tool_text if isinstance(tool_text, str) else str(tool_text)
    print_debug("Tool Output", tool_text[:300] + "... (çœç•¥)")

    return Command(
        update={
            "research_messages": result["research_messages"],
            "loop_count": state.get("loop_count", 0) + 1,
        },
        goto="research_agent",
    )


summary_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ã‚ãªãŸã¯å„ªç§€ãªæ›¸è¨˜ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œèª¿æŸ»ãƒ­ã‚°ã€ã‚’è¦ç´„ã—ã€å¸‚å ´åˆ†æžãƒãƒ¼ãƒ ãŒä½¿ãˆã‚‹åŸºç¤Žãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æ–­å®šçš„ãªäº‹å®Ÿã«ã¯å¯èƒ½ãªé™ã‚Š [n] ã‚’ä»˜ã‘ã€æœ«å°¾ã«å‚ç…§ä¸€è¦§ã‚’ä»˜ã‘ã‚‹ã€‚å­˜åœ¨ã—ãªã„å‡ºå…¸ã¯ä½œã‚‰ãªã„ã€‚",
        ),
        ("human", "ä»¥ä¸‹ãŒèª¿æŸ»ãƒ­ã‚°ã§ã™:"),
        MessagesPlaceholder(variable_name="research_messages"),
        ("human", "ä¸Šè¨˜ã‚’å…ƒã«ã€å¸‚å ´åˆ†æžã®ãŸã‚ã®åŸºç¤Žãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"),
    ]
)
summary_chain = summary_prompt | model


def summary_agent(state: State) -> dict:
    print_debug("Node", "summary_agent")
    response = summary_chain.invoke({"research_messages": state["research_messages"]})
    return {"analysis_messages": [response]}


market_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ã‚ãªãŸã¯å¸‚å ´åˆ†æžã®ãƒ—ãƒ­ã§ã™ã€‚ãƒ¬ãƒãƒ¼ãƒˆã‚’å…ƒã«SWOTåˆ†æžã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"),
        MessagesPlaceholder(variable_name="analysis_messages"),
    ]
)
market_chain = market_prompt | model


def market_agent(state: State) -> dict:
    print_debug("Node", "market_agent")
    response = market_chain.invoke({"analysis_messages": state["analysis_messages"]})
    return {"analysis_messages": [response]}


technical_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ã‚ãªãŸã¯æŠ€è¡“ã®CTOã§ã™ã€‚å¸‚å ´åˆ†æžã‚’è¸ã¾ãˆã€æŠ€è¡“çš„èª²é¡Œã¨å®Ÿç¾æ€§ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚",
        ),
        MessagesPlaceholder(variable_name="analysis_messages"),
    ]
)
technical_chain = technical_prompt | model


def technical_agent(state: State) -> dict:
    print_debug("Node", "technical_agent")
    response = technical_chain.invoke({"analysis_messages": state["analysis_messages"]})
    return {"analysis_messages": [response]}


def human_approval_node(
    state: State,
) -> Command[Literal["market_agent", "report_agent", "__end__"]]:
    payload = {
        "kind": "approval_request",
        "question": "ã“ã“ã¾ã§ã®è­°è«–ã‚’æ‰¿èªã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã‹ï¼Ÿ",
        "options": ["y", "n", "retry"],
        "analysis_preview": [
            {
                "type": type(m).__name__,
                "content": (m.content[:500] + "â€¦")
                if isinstance(m.content, str) and len(m.content) > 500
                else m.content,
            }
            for m in state.get("analysis_messages", [])
        ],
    }

    user_decision = interrupt(payload)

    if isinstance(user_decision, str):
        user_decision = user_decision.strip().lower()

    if user_decision == "y":
        return Command(goto="report_agent")
    if user_decision == "retry":
        return Command(goto="market_agent")
    return Command(goto=END)


report_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
ã“ã‚Œã¾ã§ã®è­°è«–ã‚’çµ±åˆã—ã€æŠ•è³‡å®¶å‘ã‘ã®å…·ä½“çš„ãªäº‹æ¥­ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ–‡æœ«ã§ã®è³ªå•ã‚„ææ¡ˆã¯ç¦æ­¢ã§ã™ã€‚ã€Œä»¥ä¸Šã€ã§çµ‚ã‚ã‚‰ã›ã¦ãã ã•ã„ã€‚
""",
        ),
        MessagesPlaceholder(variable_name="analysis_messages"),
    ]
)
report_chain = report_prompt | model


def report_agent(state: State) -> dict:
    print_debug("Node", "report_agent")
    response = report_chain.invoke({"analysis_messages": state["analysis_messages"]})
    return {"analysis_messages": [response]}


# ----------------------------
# Build Graph
# ----------------------------
builder = StateGraph(State)
builder.add_node("research_agent", research_agent)
builder.add_node("tools", research_tool_node)
builder.add_node("summary_agent", summary_agent)
builder.add_node("market_agent", market_agent)
builder.add_node("technical_agent", technical_agent)
builder.add_node("human_approval", human_approval_node)
builder.add_node("report_agent", report_agent)

builder.add_edge(START, "research_agent")
builder.add_edge("summary_agent", "market_agent")
builder.add_edge("market_agent", "technical_agent")
builder.add_edge("technical_agent", "human_approval")
builder.add_edge("report_agent", END)

# interrupt/resume ã‚’ä½¿ã†ãŸã‚ checkpointer ã¯å¿…é ˆï¼ˆSQLite æ°¸ç¶šåŒ–ï¼‰
CHECKPOINT_DB_PATH = os.getenv("LANGGRAPH_CHECKPOINT_DB", "checkpoints.sqlite")

# NOTE: check_same_thread=False ã¯OKï¼ˆå®Ÿè£…å´ã§ãƒ­ãƒƒã‚¯ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰å®‰å…¨ã«ã™ã‚‹æƒ³å®šï¼‰ :contentReference[oaicite:3]{index=3}
_conn = sqlite3.connect(CHECKPOINT_DB_PATH, check_same_thread=False)

# ç«¶åˆã—ã‚„ã™ã„ç’°å¢ƒï¼ˆåŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ï¼‰å‘ã‘ã«æŽ¨å¥¨ã®Pragmaï¼ˆä»»æ„ã ãŒå®Ÿé‹ç”¨ã§åŠ¹ãã‚„ã™ã„ï¼‰
_conn.execute("PRAGMA journal_mode=WAL;")
_conn.execute("PRAGMA synchronous=NORMAL;")
_conn.execute("PRAGMA busy_timeout=5000;")

checkpointer = SqliteSaver(_conn)
checkpointer.setup()  # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãªã©ã®åˆæœŸåŒ– :contentReference[oaicite:4]{index=4}

graph_app = builder.compile(checkpointer=checkpointer)


# ----------------------------
# APIå‘ã‘ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
# ----------------------------
def _as_text(m: object) -> str:
    if hasattr(m, "content"):
        c = getattr(m, "content")
        return c if isinstance(c, str) else str(c)
    return str(m)


def serialize_result(result: dict) -> dict:
    """
    LangGraphã®æˆ»ã‚Šå€¤ã«ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç­‰ãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€
    APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ã—ã¦JSONåŒ–ã§ãã‚‹å½¢ã«å¤‰æ›ã™ã‚‹ã€‚
    """
    # interrupt ã®æŠ½å‡ºï¼ˆlist/tuple/å˜ä½“ã®å·®ã‚’å¸åŽï¼‰
    interrupts = result.get("__interrupt__")
    if interrupts:
        first = interrupts[0] if isinstance(interrupts, (list, tuple)) else interrupts
        payload = getattr(first, "value", first)
        return {
            "status": "interrupted",
            "interrupt": payload,
        }

    analysis = result.get("analysis_messages", [])
    analysis_serialized = [
        {"type": type(m).__name__, "content": _as_text(m)} for m in analysis
    ]

    report_text = analysis_serialized[-1]["content"] if analysis_serialized else ""

    return {
        "status": "completed",
        "report": report_text,
        "analysis_messages": analysis_serialized,
    }


def run_graph_start(theme: str, thread_id: str) -> dict:
    initial_state: dict = {
        "research_messages": [HumanMessage(content=f"ãƒ†ãƒ¼ãƒž: {theme}")],
        "loop_count": 0,
        "analysis_messages": [],
    }
    config = {"configurable": {"thread_id": thread_id}}
    raw = graph_app.invoke(initial_state, config=config)
    return serialize_result(raw)


def run_graph_resume(decision: str, thread_id: str) -> dict:
    config = {"configurable": {"thread_id": thread_id}}
    raw = graph_app.invoke(Command(resume=decision), config=config)
    return serialize_result(raw)


def new_thread_id() -> str:
    return str(uuid.uuid4())

def close_checkpointer() -> None:
    global _conn
    try:
        _conn.close()
    except Exception:
        pass