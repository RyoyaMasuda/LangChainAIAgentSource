# graphs/agent_graph.py
from __future__ import annotations

import os
import datetime as _dt
from typing import Annotated, Literal
from typing_extensions import TypedDict

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.types import Command, interrupt

# ----------------------------
# Settings
# ----------------------------
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-5-mini")
DEBUG_MODE = os.getenv("DEBUG_MODE", "true").lower() == "true"
MAX_TOOL_LOOPS = int(os.getenv("MAX_TOOL_LOOPS", "3"))
TODAY_STR = _dt.date.today().isoformat()


def print_debug(title: str, content: str) -> None:
    if not DEBUG_MODE:
        return
    print(f"\nğŸ› [DEBUG] {title}:\n{content}\n" + "-" * 40)


# ----------------------------
# Tools
# ----------------------------
def _format_tavily_results(tavily_response: object) -> str:
    if not isinstance(tavily_response, dict):
        return f"ï¼ˆæ¤œç´¢çµæœã®å½¢å¼ãŒæƒ³å®šå¤–ã§ã—ãŸï¼‰\nraw: {tavily_response!r}"

    results = tavily_response.get("results", []) or []
    if not results:
        return "ï¼ˆæ¤œç´¢çµæœãªã—ï¼‰"

    lines: list[str] = []
    for i, r in enumerate(results, 1):
        if not isinstance(r, dict):
            continue
        title = (r.get("title") or "").strip()
        content = (r.get("content") or r.get("raw_content") or "").strip()
        url = (r.get("url") or "").strip()

        # content ãŒé•·ã™ãã‚‹ã¨ãƒ­ã‚°/ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åœ§è¿«ã™ã‚‹ã®ã§è»½ãä¸Šé™
        if len(content) > 900:
            content = content[:900] + "â€¦"

        lines.append(f"[{i}] {title}\n{content}\nsource: {url}")

    return "\n\n".join(lines) if lines else "ï¼ˆæ¤œç´¢çµæœãªã—ï¼‰"


def _build_tools():
    # Tavily ã¯ä»»æ„: ã‚­ãƒ¼ç„¡ã—ã§ã‚‚ã‚¢ãƒ—ãƒªè‡ªä½“ã¯å‹•ã‹ã™ï¼ˆæ¤œç´¢ã¯ã§ããªã„ï¼‰
    tavily_key = os.getenv("TAVILY_API_KEY", "").strip()
    if not tavily_key:

        @tool
        def tavily_search_formatted(query: str) -> str:
            """Webæ¤œç´¢ï¼ˆTavilyï¼‰ã€‚APIã‚­ãƒ¼æœªè¨­å®šã®å ´åˆã¯ãã®æ—¨ã‚’è¿”ã™ã€‚"""
            return (
                "ï¼ˆTAVILY_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ Webæ¤œç´¢ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ï¼‰\n"
                "`.env` ã« TAVILY_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
            )

        return [tavily_search_formatted]

    from langchain_tavily import TavilySearch

    tavily_search = TavilySearch(
        max_results=3,
        search_depth="basic",
        include_answer=False,
        include_raw_content=False,
        include_images=False,
    )

    @tool
    def tavily_search_formatted(query: str) -> str:
        """Webæ¤œç´¢ï¼ˆTavilyï¼‰ã€‚ä¸Šä½çµæœã‚’æ•´å½¢ã—ã¦è¿”ã™ã€‚"""
        try:
            tavily_response = tavily_search.invoke({"query": query})
            return _format_tavily_results(tavily_response)
        except Exception as e:
            return f"ï¼ˆTavilyæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ï¼‰{type(e).__name__}: {e}"

    return [tavily_search_formatted]


tools = _build_tools()


# ----------------------------
# State
# ----------------------------
class State(TypedDict, total=False):
    # add_messages reducer: updateã§æ¸¡ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ãŒæ—¢å­˜é…åˆ—ã«è¿½è¨˜ã•ã‚Œã‚‹
    research_messages: Annotated[list[BaseMessage], add_messages]
    analysis_messages: Annotated[list[BaseMessage], add_messages]
    loop_count: int

    # UIå‘ã‘ï¼ˆé–‹å§‹æ™‚ã«ç«‹ã¦ã‚‹ï¼‰
    current_step: str
    approval_decision: str

    # report_agent ã®ã¿ãŒã‚»ãƒƒãƒˆã™ã‚‹ã€Œæœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã€
    final_report: str


# ----------------------------
# Model / Chains
# ----------------------------
model = ChatOpenAI(model=MODEL_NAME, temperature=0)
model_with_tools = model.bind_tools(tools)

research_prompt_text = f"""
ã‚ãªãŸã¯äº‹æ¥­ãƒªã‚µãƒ¼ãƒæ‹…å½“ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€å¸‚å ´è¦æ¨¡ã€ä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€æŠ€è¡“èª²é¡Œãªã©ã‚’Webæ¤œç´¢ã§èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚
ä»Šæ—¥ã¯ {TODAY_STR} ã§ã™ã€‚æ–°ã—ã„æƒ…å ±ãŒå¿…è¦ãªã‚‰æ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
å¿…è¦ã«å¿œã˜ã¦æœ€é©ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºå…¸ãƒ«ãƒ¼ãƒ«ã€‘
- ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ã«ã‚ã‚‹ "source: URL" ã‚’æ ¹æ‹ ã¨ã—ã¦ä½¿ã†ã¨ãã®ã¿ã€æœ¬æ–‡ä¸­ã« [n] ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
- æœ«å°¾ã«å‚ç…§ä¸€è¦§ï¼ˆ[n] URLï¼‰ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
- å­˜åœ¨ã—ãªã„å‡ºå…¸ã¯ä½œã‚‰ãªã„ã“ã¨ã€‚
""".strip()

research_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", research_prompt_text),
        MessagesPlaceholder(variable_name="research_messages"),
    ]
)
research_chain = research_prompt | model_with_tools

summary_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ã‚ãªãŸã¯å„ªç§€ãªæ›¸è¨˜ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œèª¿æŸ»ãƒ­ã‚°ã€ã‚’è¦ç´„ã—ã€å¸‚å ´åˆ†æãƒãƒ¼ãƒ ãŒä½¿ãˆã‚‹åŸºç¤ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
            "æ–­å®šçš„ãªäº‹å®Ÿã«ã¯å¯èƒ½ãªé™ã‚Š [n] ã‚’ä»˜ã‘ã€æœ«å°¾ã«å‚ç…§ä¸€è¦§ï¼ˆ[n] URLï¼‰ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚å­˜åœ¨ã—ãªã„å‡ºå…¸ã¯ä½œã‚‰ãªã„ã€‚\n"
            "ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ã«å«ã¾ã‚Œã‚‹ source: URL ã ã‘ã‚’å‚ç…§ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„ã€‚",
        ),
        ("human", "ä»¥ä¸‹ãŒèª¿æŸ»ãƒ­ã‚°ã§ã™:"),
        MessagesPlaceholder(variable_name="research_messages"),
        ("human", "ä¸Šè¨˜ã‚’å…ƒã«ã€å¸‚å ´åˆ†æã®ãŸã‚ã®åŸºç¤ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"),
    ]
)
summary_chain = summary_prompt | model

market_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ã‚ãªãŸã¯å¸‚å ´åˆ†æã®ãƒ—ãƒ­ã§ã™ã€‚ãƒ¬ãƒãƒ¼ãƒˆã‚’å…ƒã«SWOTåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"),
        MessagesPlaceholder(variable_name="analysis_messages"),
    ]
)
market_chain = market_prompt | model

technical_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ã‚ãªãŸã¯æŠ€è¡“ã®CTOã§ã™ã€‚å¸‚å ´åˆ†æã‚’è¸ã¾ãˆã€æŠ€è¡“çš„èª²é¡Œã¨å®Ÿç¾æ€§ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚"),
        MessagesPlaceholder(variable_name="analysis_messages"),
    ]
)
technical_chain = technical_prompt | model

report_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ã“ã‚Œã¾ã§ã®è­°è«–ã‚’çµ±åˆã—ã€æŠ•è³‡å®¶å‘ã‘ã®å…·ä½“çš„ãªäº‹æ¥­ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
            "æ–‡æœ«ã§ã®è³ªå•ã‚„ææ¡ˆã¯ç¦æ­¢ã§ã™ã€‚ã€Œä»¥ä¸Šã€ã§çµ‚ã‚ã‚‰ã›ã¦ãã ã•ã„ã€‚",
        ),
        MessagesPlaceholder(variable_name="analysis_messages"),
    ]
)
report_chain = report_prompt | model


# ----------------------------
# Utility: start-marker nodes
# ----------------------------
def _mark_step(step: str, goto: str) -> Command:
    # ã€Œé–‹å§‹æ™‚ã€ã« step ã‚’æ›´æ–°ã™ã‚‹è»½é‡ãƒãƒ¼ãƒ‰
    return Command(update={"current_step": step}, goto=goto)


# ----------------------------
# Nodes
# ----------------------------
def research_start(state: State) -> Command[Literal["research_agent"]]:
    print_debug("Node", "research_start")
    return _mark_step("research_agent", "research_agent")


def research_agent(state: State) -> Command[Literal["tools_start", "summary_start"]]:
    print_debug("Node", "research_agent")

    response = research_chain.invoke({"research_messages": state.get("research_messages", [])})
    update = {"research_messages": [response]}
    current_count = state.get("loop_count", 0)

    # LLMãŒãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è¦æ±‚ã—ãŸã‚‰ tools ã¸
    if getattr(response, "tool_calls", None):
        if current_count < MAX_TOOL_LOOPS:
            return Command(update=update, goto="tools_start")
        return Command(update=update, goto="summary_start")

    return Command(update=update, goto="summary_start")


def tools_start(state: State) -> Command[Literal["tools"]]:
    print_debug("Node", "tools_start")
    return _mark_step("tools", "tools")


tool_node = ToolNode(tools, messages_key="research_messages")


def research_tool_node(state: State) -> Command[Literal["research_start"]]:
    print_debug("Node", "tools")

    result = tool_node.invoke({"research_messages": state.get("research_messages", [])})

    last_message = result["research_messages"][-1]
    tool_text = last_message.content
    tool_text = tool_text if isinstance(tool_text, str) else str(tool_text)
    print_debug("Tool Output", tool_text[:300] + ("... (çœç•¥)" if len(tool_text) > 300 else ""))

    return Command(
        update={
            "research_messages": result["research_messages"],
            "loop_count": state.get("loop_count", 0) + 1,
        },
        goto="research_start",
    )


def summary_start(state: State) -> Command[Literal["summary_agent"]]:
    print_debug("Node", "summary_start")
    return _mark_step("summary_agent", "summary_agent")


def summary_agent(state: State) -> Command[Literal["market_start"]]:
    print_debug("Node", "summary_agent")
    response = summary_chain.invoke({"research_messages": state.get("research_messages", [])})
    return Command(
        update={"analysis_messages": [response], "loop_count": 0},
        goto="market_start",
    )


def market_start(state: State) -> Command[Literal["market_agent"]]:
    print_debug("Node", "market_start")
    return _mark_step("market_agent", "market_agent")


def market_agent(state: State) -> Command[Literal["technical_start"]]:
    print_debug("Node", "market_agent")
    response = market_chain.invoke({"analysis_messages": state.get("analysis_messages", [])})
    return Command(update={"analysis_messages": [response]}, goto="technical_start")


def technical_start(state: State) -> Command[Literal["technical_agent"]]:
    print_debug("Node", "technical_start")
    return _mark_step("technical_agent", "technical_agent")


def technical_agent(state: State) -> Command[Literal["human_approval_start"]]:
    print_debug("Node", "technical_agent")
    response = technical_chain.invoke({"analysis_messages": state.get("analysis_messages", [])})
    return Command(update={"analysis_messages": [response]}, goto="human_approval_start")


def human_approval_start(state: State) -> Command[Literal["human_approval"]]:
    print_debug("Node", "human_approval_start")
    return _mark_step("human_approval", "human_approval")


def _safe_preview_messages(messages: list[BaseMessage], limit: int = 3) -> list[dict]:
    out: list[dict] = []
    tail = messages[-limit:] if limit > 0 else messages
    for m in tail:
        content = m.content
        s = content if isinstance(content, str) else str(content)
        if len(s) > 1200:
            s = s[:1200] + "â€¦"
        out.append({"type": type(m).__name__, "content": s})
    return out


def human_approval_node(
    state: State,
) -> Command[Literal["market_start", "report_start", "__end__"]]:
    """
    HITLï¼ˆæ‰¿èªï¼‰ãƒãƒ¼ãƒ‰ã€‚
    - interrupt(payload) ã§ç¢ºå®Ÿã«åœæ­¢
    - resume ã•ã‚ŒãŸå€¤ã¯ approval_decision ã«ä¿å­˜
    - decision ãŒ y ä»¥å¤–ãªã‚‰ report ã¸è¡Œã‹ãªã„
    """
    print_debug("Node", "human_approval")

    payload = {
        "kind": "approval_request",
        "question": "ã“ã“ã¾ã§ã®è­°è«–ã‚’æ‰¿èªã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã‹ï¼Ÿ",
        "options": ["y", "n", "retry"],
        "analysis_preview": _safe_preview_messages(state.get("analysis_messages", []), limit=3),
    }

    user_decision = interrupt(payload)
    print_debug("Approval decision (raw)", repr(user_decision))

    if isinstance(user_decision, str):
        decision_str = user_decision.strip().lower()
    else:
        decision_str = str(user_decision).strip().lower()

    if decision_str not in ("y", "n", "retry"):
        decision_str = "n"

    update = {"approval_decision": decision_str}

    if decision_str == "y":
        return Command(update=update, goto="report_start")
    if decision_str == "retry":
        return Command(update=update, goto="market_start")
    return Command(update=update, goto=END)


def report_start(state: State) -> Command[Literal["report_agent"]]:
    print_debug("Node", "report_start")
    return _mark_step("report_agent", "report_agent")


def report_agent(state: State) -> Command[Literal["__end__"]]:
    print_debug("Node", "report_agent")

    if (state.get("approval_decision") or "").strip().lower() != "y":
        return Command(update={"final_report": ""}, goto=END)

    response = report_chain.invoke({"analysis_messages": state.get("analysis_messages", [])})
    text = response.content if isinstance(response.content, str) else str(response.content)

    return Command(update={"analysis_messages": [response], "final_report": text}, goto=END)


# ----------------------------
# Build Graph
# ----------------------------
builder = StateGraph(State)

builder.add_node("research_start", research_start)
builder.add_node("research_agent", research_agent)

builder.add_node("tools_start", tools_start)
builder.add_node("tools", research_tool_node)

builder.add_node("summary_start", summary_start)
builder.add_node("summary_agent", summary_agent)

builder.add_node("market_start", market_start)
builder.add_node("market_agent", market_agent)

builder.add_node("technical_start", technical_start)
builder.add_node("technical_agent", technical_agent)

builder.add_node("human_approval_start", human_approval_start)
builder.add_node("human_approval", human_approval_node)

builder.add_node("report_start", report_start)
builder.add_node("report_agent", report_agent)

builder.add_edge(START, "research_start")

# ä»¥é™ã¯ Command ã® goto ã§é·ç§»ï¼ˆæ˜ç¤ºedgeã¯æœ€å°åŒ–ï¼‰
builder.add_edge("report_agent", END)

graph = builder.compile()
